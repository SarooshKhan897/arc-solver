{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ uvloop enabled\n",
            "âœ“ Imports loaded\n",
            "âœ“ Max workers: 100\n",
            "âœ“ Solver models: ['gpt-5.2', 'gemini-pro', 'gemini-flash']\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# SETUP - Run this cell first!\n",
        "# ============================================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import random\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Jupyter async compatibility\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Try uvloop for performance\n",
        "try:\n",
        "    import uvloop\n",
        "    uvloop.install()\n",
        "    print(\"âœ“ uvloop enabled\")\n",
        "except ImportError:\n",
        "    print(\"â„¹ uvloop not available\")\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "# Import our solver\n",
        "from src.run import solve_task, evaluate_task, run_tasks\n",
        "from src.models import ArcEvaluator\n",
        "from src.config import MAX_WORKERS, SOLVER_MODELS\n",
        "\n",
        "print(\"âœ“ Imports loaded\")\n",
        "print(f\"âœ“ Max workers: {MAX_WORKERS}\")\n",
        "print(f\"âœ“ Solver models: {[m['id'] for m in SOLVER_MODELS]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from src.perception.analyzer import analyze_task, analyze_grid, analyze_example\n",
        "\n",
        "# ðŸ‘‡ CHANGE THIS PATH TO ANALYZE ANY TASK\n",
        "ANALYZE_TASK_PATH = \"../ARC-AGI-2/data/evaluation/20a9e565.json\"\n",
        "\n",
        "def print_task_analysis(task_path: str):\n",
        "    \"\"\"Print comprehensive task analysis.\"\"\"\n",
        "    with open(task_path) as f:\n",
        "        task_data = json.load(f)\n",
        "    \n",
        "    task_id = os.path.basename(task_path).replace('.json', '')\n",
        "    analysis = analyze_task(task_data, task_id)\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    print(f\"ðŸ“‹ Task Details\")\n",
        "    print(f\"   Task ID: {task_id}\")\n",
        "    print(f\"   Dataset: ARC-AGI-2 Evaluation\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    for ex in analysis.train_examples:\n",
        "        inp = ex.input_analysis\n",
        "        out = ex.output_analysis\n",
        "        trans = ex.transform_analysis\n",
        "        \n",
        "        print(f\"\\n{'â”€'*70}\")\n",
        "        print(f\"ðŸ“š Training Example #{ex.example_num}\")\n",
        "        print(f\"{'â”€'*70}\")\n",
        "        \n",
        "        print(f\"\\nðŸ”¹ INPUT\")\n",
        "        print(f\"   Grid Size:      {inp.rows} Ã— {inp.cols} ({inp.total_cells} cells)\")\n",
        "        print(f\"   Colors Used:    {inp.colors_used} colors\")\n",
        "        print(f\"   Color Palette:  {', '.join(inp.color_palette)}\")\n",
        "        print(f\"   Background:     {inp.background_color}\")\n",
        "        print(f\"   Fill Ratio:     {inp.fill_ratio:.1f}%\")\n",
        "        print(f\"   Shapes by Color:\")\n",
        "        for color, count in inp.shapes_by_color.items():\n",
        "            print(f\"      {color}: {count}\")\n",
        "        \n",
        "        print(f\"\\nðŸ”¹ OUTPUT\")\n",
        "        print(f\"   Grid Size:      {out.rows} Ã— {out.cols} ({out.total_cells} cells)\")\n",
        "        print(f\"   Colors Used:    {out.colors_used} colors\")\n",
        "        print(f\"   Color Palette:  {', '.join(out.color_palette)}\")\n",
        "        print(f\"   Background:     {out.background_color}\")\n",
        "        print(f\"   Fill Ratio:     {out.fill_ratio:.1f}%\")\n",
        "        print(f\"   Shapes by Color:\")\n",
        "        for color, count in out.shapes_by_color.items():\n",
        "            print(f\"      {color}: {count}\")\n",
        "        \n",
        "        print(f\"\\nðŸ”„ KEY TRANSFORMATIONS\")\n",
        "        print(f\"   Size Change:         {inp.total_cells} cells â†’ {out.total_cells} cells ({trans.size_change_percent:+.1f}%)\")\n",
        "        print(f\"   New Colors Intro:    {'YES' if trans.new_colors_introduced else 'NO'}\")\n",
        "        print(f\"   Density Change:      {inp.fill_ratio:.1f}% â†’ {out.fill_ratio:.1f}% ({trans.density_change_percent:+.1f}%)\")\n",
        "        print(f\"   Shape Count:         {trans.input_shape_count} shapes â†’ {trans.output_shape_count} shapes\")\n",
        "        print(f\"   Transform Hints:     {', '.join(trans.hints)}\")\n",
        "    \n",
        "    # Test inputs (we predict outputs for these - outputs are NOT shown to solver)\n",
        "    for i, test in enumerate(analysis.test_inputs, 1):\n",
        "        print(f\"\\n{'â”€'*70}\")\n",
        "        label = f\"ðŸ§ª TEST INPUT #{i}\" if len(analysis.test_inputs) > 1 else \"ðŸ§ª TEST INPUT\"\n",
        "        print(f\"{label} (predict output for this)\")\n",
        "        print(f\"{'â”€'*70}\")\n",
        "        \n",
        "        print(f\"\\nðŸ”¹ INPUT\")\n",
        "        print(f\"   Grid Size:      {test.rows} Ã— {test.cols} ({test.total_cells} cells)\")\n",
        "        print(f\"   Colors Used:    {test.colors_used} colors\")\n",
        "        print(f\"   Color Palette:  {', '.join(test.color_palette)}\")\n",
        "        print(f\"   Background:     {test.background_color}\")\n",
        "        print(f\"   Fill Ratio:     {test.fill_ratio:.1f}%\")\n",
        "        print(f\"   Shapes by Color:\")\n",
        "        for color, count in test.shapes_by_color.items():\n",
        "            print(f\"      {color}: {count}\")\n",
        "    \n",
        "    # Cross-example patterns\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ðŸ” CROSS-EXAMPLE PATTERNS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"   Size always preserved:   {analysis.consistent_size_preservation}\")\n",
        "    print(f\"   Colors always preserved: {analysis.consistent_color_preservation}\")\n",
        "    print(f\"   Shape count preserved:   {analysis.consistent_shape_count}\")\n",
        "    if analysis.common_hints:\n",
        "        print(f\"   Common hints:            {', '.join(analysis.common_hints)}\")\n",
        "\n",
        "# Run it!\n",
        "print_task_analysis(ANALYZE_TASK_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# RUN A SINGLE TASK BY PATH\n",
        "# ============================================================================\n",
        "\n",
        "import asyncio\n",
        "\n",
        "# ðŸ‘‡ CHANGE THIS PATH TO RUN ANY TASK\n",
        "TASK_PATH = \"../ARC-AGI-2/data/evaluation/581f7754.json\"\n",
        "\n",
        "async def run_single():\n",
        "    # Load task\n",
        "    with open(TASK_PATH) as f:\n",
        "        task_data = json.load(f)\n",
        "    \n",
        "    task_name = os.path.basename(TASK_PATH)\n",
        "    n_tests = len(task_data['test'])\n",
        "    \n",
        "    # Check for ground truths (ALL test cases)\n",
        "    has_ground_truth = 'output' in task_data['test'][0]\n",
        "    ground_truths = [np.array(t['output']) for t in task_data['test']] if has_ground_truth else []\n",
        "    \n",
        "    # Prepare for solver - pass ALL test inputs (without outputs)\n",
        "    task_for_solver = {\n",
        "        'train': task_data['train'],\n",
        "        'test': [{'input': t['input']} for t in task_data['test']],\n",
        "    }\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ðŸ“‹ Task: {task_name}\")\n",
        "    print(f\"   Training examples: {len(task_data['train'])}\")\n",
        "    print(f\"   Test inputs: {n_tests}\")\n",
        "    print(f\"   Ground truth: {'âœ“ Available' if has_ground_truth else 'âœ— Not available'}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    # Solve! - now returns predictions_per_test[test_idx][attempt_idx]\n",
        "    predictions_per_test, info = await solve_task(task_for_solver, verbose=True)\n",
        "    \n",
        "    duration = (datetime.now() - start_time).total_seconds()\n",
        "    \n",
        "    # Evaluate using official ARC scoring\n",
        "    if ground_truths:\n",
        "        n_correct = 0\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"ðŸ“Š EVALUATION (Official ARC Scoring)\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        for test_idx, gt in enumerate(ground_truths):\n",
        "            attempts = predictions_per_test[test_idx]\n",
        "            match1 = np.array_equal(np.array(attempts[0]), gt)\n",
        "            match2 = np.array_equal(np.array(attempts[1]), gt) if len(attempts) > 1 else False\n",
        "            correct = match1 or match2\n",
        "            \n",
        "            if correct:\n",
        "                n_correct += 1\n",
        "            \n",
        "            if n_tests > 1:\n",
        "                print(f\"\\n   Test {test_idx + 1}/{n_tests}:\")\n",
        "                print(f\"      Attempt 1: {'âœ… CORRECT' if match1 else 'âŒ Wrong'}\")\n",
        "                print(f\"      Attempt 2: {'âœ… CORRECT' if match2 else 'âŒ Wrong'}\")\n",
        "                print(f\"      Result: {'âœ“ PASS' if correct else 'âœ— FAIL'}\")\n",
        "            else:\n",
        "                print(f\"   Attempt 1: {'âœ… CORRECT' if match1 else 'âŒ Wrong'}\")\n",
        "                print(f\"   Attempt 2: {'âœ… CORRECT' if match2 else 'âŒ Wrong'}\")\n",
        "        \n",
        "        score = n_correct / n_tests\n",
        "        print(f\"\\n   {'â”€' * 40}\")\n",
        "        print(f\"   ðŸ“Š Task Score: {n_correct}/{n_tests} ({score*100:.0f}%)\")\n",
        "        print(f\"   {'ðŸŽ‰ TASK SOLVED!' if score == 1.0 else 'âŒ Task not fully solved'}\")\n",
        "        print(f\"   â±ï¸  Time: {duration:.1f}s\")\n",
        "    else:\n",
        "        print(f\"\\nâ±ï¸  Completed in {duration:.1f}s (no ground truth)\")\n",
        "    \n",
        "    return predictions_per_test, info\n",
        "\n",
        "# Run it!\n",
        "predictions_per_test, info = asyncio.get_event_loop().run_until_complete(run_single())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# RUN N RANDOM TASKS FROM EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "import asyncio\n",
        "\n",
        "# ðŸ‘‡ CONFIG\n",
        "NUM_TASKS = 5  # Number of random tasks to run\n",
        "EVAL_DIR = \"../ARC-AGI-2/data/evaluation\"\n",
        "\n",
        "async def run_random_eval():\n",
        "    # Get all tasks\n",
        "    all_tasks = sorted(glob.glob(os.path.join(EVAL_DIR, \"*.json\")))\n",
        "    \n",
        "    # Random sample\n",
        "    random.seed()  # Use current time for randomness\n",
        "    selected = random.sample(all_tasks, min(NUM_TASKS, len(all_tasks)))\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ðŸŽ² RANDOM EVALUATION ({len(selected)} tasks)\")\n",
        "    print(f\"   Available: {len(all_tasks)} tasks\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nðŸ“‹ Selected: {[os.path.basename(p) for p in selected[:5]]}...\")\n",
        "    \n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    # Run tasks\n",
        "    results = await run_tasks(selected, verbose=True)\n",
        "    \n",
        "    duration = (datetime.now() - start_time).total_seconds()\n",
        "    \n",
        "    # Summary using official ARC scoring\n",
        "    total_tests = sum(r.n_test_cases for r in results)\n",
        "    total_correct = sum(r.n_correct for r in results)\n",
        "    tasks_fully_solved = sum(1 for r in results if r.score == 1.0)\n",
        "    errors = sum(1 for r in results if r.error)\n",
        "    avg_score = sum(r.score for r in results) / len(results) if results else 0\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ðŸ† RESULTS (Official ARC Scoring)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ðŸ“‹ Tasks evaluated: {len(results)}\")\n",
        "    print(f\"ðŸ“‹ Total test cases: {total_tests}\")\n",
        "    print(f\"âœ… Test cases correct: {total_correct}/{total_tests} ({100*total_correct/total_tests:.1f}%)\")\n",
        "    print(f\"ðŸŽ¯ Tasks fully solved: {tasks_fully_solved}/{len(results)}\")\n",
        "    print(f\"ðŸ“Š Average task score: {avg_score*100:.1f}%\")\n",
        "    print(f\"âŒ Errors: {errors}\")\n",
        "    print(f\"â±ï¸  Total time: {duration:.0f}s ({duration/len(results):.1f}s per task avg)\")\n",
        "    \n",
        "    print(\"\\nðŸ“‹ Individual results:\")\n",
        "    for r in results:\n",
        "        status = \"âœ“\" if r.score == 1.0 else \"â—\" if r.score > 0 else \"âœ—\"\n",
        "        tests_info = f\" ({r.n_correct}/{r.n_test_cases} tests)\" if r.n_test_cases > 1 else \"\"\n",
        "        print(f\"   {status} {r.task_id}: {r.score*100:.0f}%{tests_info}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run it!\n",
        "results = asyncio.get_event_loop().run_until_complete(run_random_eval())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Found 83/83 valid tasks in fail_tasks.txt\n",
            "============================================================\n",
            "ðŸ”„ RETRYING FAILED TASKS (5 tasks)\n",
            "   Total failed: 83 tasks\n",
            "============================================================\n",
            "\n",
            "ðŸ“‹ Selected: ['247ef758.json', 'de809cff.json', 'da515329.json', '221dfab4.json', 'edb79dae.json']...\n",
            "ðŸš€ Running 5 tasks (max 100 concurrent)\n",
            "\n",
            "ðŸš€ [1/5] Starting: 247ef758.json\n",
            "   ðŸ“‹ Task has 2 test inputs\n",
            "============================================================\n",
            "ðŸŽ­ ARC SOLVER\n",
            "============================================================\n",
            "   Training examples: 3\n",
            "   Test inputs: 2\n",
            "------------------------------------------------------------\n",
            "  ðŸ‘ï¸ Perceiving grids...\n",
            "\n",
            "ðŸš€ [2/5] Starting: de809cff.json\n",
            "============================================================\n",
            "ðŸŽ­ ARC SOLVER\n",
            "============================================================\n",
            "   Training examples: 2\n",
            "   Test inputs: 1\n",
            "------------------------------------------------------------\n",
            "  ðŸ‘ï¸ Perceiving grids...\n",
            "\n",
            "ðŸš€ [3/5] Starting: da515329.json\n",
            "============================================================\n",
            "ðŸŽ­ ARC SOLVER\n",
            "============================================================\n",
            "   Training examples: 3\n",
            "   Test inputs: 1\n",
            "------------------------------------------------------------\n",
            "  ðŸ‘ï¸ Perceiving grids...\n",
            "\n",
            "ðŸš€ [4/5] Starting: 221dfab4.json\n",
            "   ðŸ“‹ Task has 2 test inputs\n",
            "============================================================\n",
            "ðŸŽ­ ARC SOLVER\n",
            "============================================================\n",
            "   Training examples: 2\n",
            "   Test inputs: 2\n",
            "------------------------------------------------------------\n",
            "  ðŸ‘ï¸ Perceiving grids...\n",
            "\n",
            "ðŸš€ [5/5] Starting: edb79dae.json\n",
            "============================================================\n",
            "ðŸŽ­ ARC SOLVER\n",
            "============================================================\n",
            "   Training examples: 2\n",
            "   Test inputs: 1\n",
            "------------------------------------------------------------\n",
            "  ðŸ‘ï¸ Perceiving grids...\n",
            "     âœ“ Perceived 8 grids\n",
            "     âœ“ Perceived 8 grids (6 training + 2 test)\n",
            "  ðŸ” Computing deltas...\n",
            "     âœ“ Perceived 6 grids\n",
            "     âœ“ Perceived 6 grids (4 training + 2 test)\n",
            "  ðŸ” Computing deltas...\n",
            "     âœ“ Perceived 5 grids\n",
            "     âœ“ Perceived 5 grids (4 training + 1 test)\n",
            "  ðŸ” Computing deltas...\n",
            "     âœ“ Perceived 5 grids\n",
            "     âœ“ Perceived 5 grids (4 training + 1 test)\n",
            "  ðŸ” Computing deltas...\n",
            "     âœ“ Perceived 7 grids\n",
            "     âœ“ Perceived 7 grids (6 training + 1 test)\n",
            "  ðŸ” Computing deltas...\n",
            "     âœ“ Computed 3 deltas\n",
            "     âœ“ Computed 3 deltas\n",
            "  ðŸ”® Perceiver analyzing task for hypotheses...\n",
            "  ðŸ‘ï¸ Perceiver analyzing task...\n",
            "     âœ“ Computed 2 deltas\n",
            "     âœ“ Computed 2 deltas\n",
            "  ðŸ”® Perceiver analyzing task for hypotheses...\n",
            "  ðŸ‘ï¸ Perceiver analyzing task...\n",
            "     âœ“ Computed 2 deltas\n",
            "     âœ“ Computed 2 deltas\n",
            "  ðŸ”® Perceiver analyzing task for hypotheses...\n",
            "  ðŸ‘ï¸ Perceiver analyzing task...\n",
            "     âœ“ Computed 2 deltas\n",
            "     âœ“ Computed 2 deltas\n",
            "  ðŸ”® Perceiver analyzing task for hypotheses...\n",
            "  ðŸ‘ï¸ Perceiver analyzing task...\n",
            "     âœ“ Computed 3 deltas\n",
            "     âœ“ Computed 3 deltas\n",
            "  ðŸ”® Perceiver analyzing task for hypotheses...\n",
            "  ðŸ‘ï¸ Perceiver analyzing task...\n",
            "     âœ“ Generated 5 transformation hypotheses\n",
            "     ðŸ’¡ Key insight: The grid is a coordinate system where the left side is a 'pa...\n",
            "     âœ“ Perceiver generated 5 transformation hypotheses\n",
            "     ðŸ’¡ Key insight: The grid is a coordinate system where the left side is a 'pa...\n",
            "------------------------------------------------------------\n",
            "  ðŸš€ Running parallel model execution...\n",
            "  ðŸš€ Launching 3 models in parallel...\n",
            "     ðŸš€ [gpt-5.2] Starting (5 tries)...\n",
            "     ðŸš€ [gemini-pro] Starting (10 tries)...\n",
            "     ðŸš€ [gemini-flash] Starting (10 tries)...\n",
            "     âœ“ Generated 5 transformation hypotheses\n",
            "     ðŸ’¡ Key insight: The input grid contains a self-documenting 'legend' outside ...\n",
            "     âœ“ Perceiver generated 5 transformation hypotheses\n",
            "     ðŸ’¡ Key insight: The input grid contains a self-documenting 'legend' outside ...\n",
            "------------------------------------------------------------\n",
            "  ðŸš€ Running parallel model execution...\n",
            "  ðŸš€ Launching 3 models in parallel...\n",
            "     ðŸš€ [gpt-5.2] Starting (5 tries)...\n",
            "     ðŸš€ [gemini-pro] Starting (10 tries)...\n",
            "     ðŸš€ [gemini-flash] Starting (10 tries)...\n",
            "     [gemini-flash] Attempt 1/10: âœ— Failed training\n",
            "     [gemini-flash] Attempt 1/10: âœ— No code found\n",
            "     âœ“ Generated 5 transformation hypotheses\n",
            "     ðŸ’¡ Key insight: The transformation applies a vertical 'beam' defined by the ...\n",
            "     âœ“ Perceiver generated 5 transformation hypotheses\n",
            "     ðŸ’¡ Key insight: The transformation applies a vertical 'beam' defined by the ...\n",
            "------------------------------------------------------------\n",
            "  ðŸš€ Running parallel model execution...\n",
            "  ðŸš€ Launching 3 models in parallel...\n",
            "     ðŸš€ [gpt-5.2] Starting (5 tries)...\n",
            "     ðŸš€ [gemini-pro] Starting (10 tries)...\n",
            "     ðŸš€ [gemini-flash] Starting (10 tries)...\n",
            "     âœ“ Generated 5 transformation hypotheses\n",
            "     ðŸ’¡ Key insight: The transformation involves a toroidal shift of the grid by ...\n",
            "     âœ“ Perceiver generated 5 transformation hypotheses\n",
            "     ðŸ’¡ Key insight: The transformation involves a toroidal shift of the grid by ...\n",
            "------------------------------------------------------------\n",
            "  ðŸš€ Running parallel model execution...\n",
            "  ðŸš€ Launching 3 models in parallel...\n",
            "     ðŸš€ [gpt-5.2] Starting (5 tries)...\n",
            "     ðŸš€ [gemini-pro] Starting (10 tries)...\n",
            "     ðŸš€ [gemini-flash] Starting (10 tries)...\n",
            "     [gemini-pro] Attempt 1/10: âœ— Failed training\n",
            "     âœ“ Generated 5 transformation hypotheses\n",
            "     ðŸ’¡ Key insight: The input shape (a hollow cross) acts as a seed that is pres...\n",
            "     âœ“ Perceiver generated 5 transformation hypotheses\n",
            "     ðŸ’¡ Key insight: The input shape (a hollow cross) acts as a seed that is pres...\n",
            "------------------------------------------------------------\n",
            "  ðŸš€ Running parallel model execution...\n",
            "  ðŸš€ Launching 3 models in parallel...\n",
            "     ðŸš€ [gpt-5.2] Starting (5 tries)...\n",
            "     ðŸš€ [gemini-pro] Starting (10 tries)...\n",
            "     ðŸš€ [gemini-flash] Starting (10 tries)...\n",
            "     [gemini-pro] Attempt 1/10: âœ“ Passed training\n",
            "     [gemini-pro] Applied transform to 2 test inputs\n",
            "     [gemini-pro] ðŸ” Self-verification with gpt-5.2 (2 test(s))...\n",
            "     [gemini-flash] Attempt 2/10: âœ— Failed training\n",
            "     [gemini-pro] Attempt 1/10: âœ— Failed training\n",
            "     [gemini-flash] Attempt 1/10: âœ“ Passed training\n",
            "     [gemini-flash] Applied transform to 2 test inputs\n",
            "     [gemini-flash] ðŸ” Self-verification with gpt-5.2 (2 test(s))...\n",
            "     [gemini-flash] Attempt 2/10: âœ“ Passed training\n",
            "     [gemini-flash] Applied transform to 2 test inputs\n",
            "     [gemini-flash] ðŸ” Self-verification with gpt-5.2 (2 test(s))...\n",
            "     [gemini-flash] Attempt 1/10: âœ— Failed training\n",
            "     [gemini-flash] Attempt 3/10: âœ— Failed training\n",
            "     [gemini-pro] Attempt 2/10: âœ— Failed training\n",
            "     [gemini-flash] Attempt 1/10: âœ— Failed training\n",
            "     [gemini-pro] Attempt 2/10: âœ“ Passed training\n",
            "     [gemini-pro] Applied transform to 2 test inputs\n",
            "     [gemini-pro] ðŸ” Self-verification with gpt-5.2 (2 test(s))...\n",
            "    ðŸ” Self-verify: âš ï¸ WRONG (score=35)\n",
            "     [gemini-pro] ðŸ” Self-verify: âš ï¸ WRONG (score=35)\n",
            "     [gemini-pro] Attempt 1/10: âœ— Failed training\n",
            "     [gemini-flash] Attempt 4/10: âœ— Failed training\n",
            "     [gemini-pro] Attempt 1/10: âœ— Failed training\n",
            "     [gemini-flash] Attempt 2/10: âœ— Failed training\n",
            "     [gemini-pro] Attempt 3/10: âœ— Failed training\n",
            "     [gemini-pro] Attempt 2/10: âœ“ Passed training\n",
            "     [gemini-pro] Applied transform to 2 test inputs\n",
            "     [gemini-pro] ðŸ” Self-verification with gpt-5.2 (2 test(s))...\n",
            "    ðŸ” Self-verify: âš ï¸ UNSURE (score=58)\n",
            "     [gemini-flash] ðŸ” Self-verify: âš ï¸ UNSURE (score=58)\n",
            "     [gpt-5.2] Attempt 1/5: âœ“ Passed training\n",
            "     [gpt-5.2] Applied transform to 2 test inputs\n",
            "     [gpt-5.2] ðŸ” Self-verification with gpt-5.2 (2 test(s))...\n",
            "     [gemini-flash] Attempt 5/10: âœ— Failed training\n",
            "    ðŸ” Self-verify: âœ“ CORRECT (score=92)\n",
            "     [gemini-flash] âœ… Self-verify=CORRECT (score=92)\n",
            "     [gemini-flash] Generated 2 test outputs\n",
            "  ðŸ”¥ [gemini-flash] Self-verified solution! (score=92)\n",
            "     [gemini-flash] Attempt 2/10: âœ— Failed training\n",
            "     [gemini-flash] Attempt 3/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 4/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 5/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 6/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 7/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 8/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 9/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 10/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] âŒ Exhausted 10 tries\n",
            "     [gemini-flash] Attempt 2/10: âœ“ Passed training\n",
            "     [gemini-flash] Applied transform to 2 test inputs\n",
            "     [gemini-flash] ðŸ” Self-verification with gpt-5.2 (2 test(s))...\n",
            "     [gemini-flash] Attempt 2/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 3/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 4/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 5/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 6/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 7/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 8/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 9/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 10/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] âŒ Exhausted 10 tries\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Run it!\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m results = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_failed_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/learning/.venv/lib/python3.13/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/learning/.venv/lib/python3.13/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/selectors.py:548\u001b[39m, in \u001b[36mKqueueSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    546\u001b[39m ready = []\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     kev_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     [gemini-flash] Attempt 3/10: âœ— Failed training\n",
            "     [gemini-flash] Attempt 4/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 5/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 6/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 7/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 8/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 9/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 10/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] âŒ Exhausted 10 tries\n",
            "     [gemini-flash] Attempt 6/10: âœ— Failed training\n",
            "     [gemini-flash] Attempt 7/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 8/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 9/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] Attempt 10/10: âœ— Error - Error code: 401 - {'error': {'message': 'User not \n",
            "     [gemini-flash] âŒ Exhausted 10 tries\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# RUN N RANDOM TASKS FROM FAILED LIST\n",
        "# ============================================================================\n",
        "\n",
        "import asyncio\n",
        "\n",
        "# ðŸ‘‡ CONFIG\n",
        "NUM_TASKS = 5  # Number of failed tasks to retry\n",
        "FAIL_TASKS_FILE = \"../fail_tasks.txt\"\n",
        "EVAL_DIR = \"../ARC-AGI-2/data/evaluation\"\n",
        "\n",
        "async def run_failed_tasks():\n",
        "    # Read failed tasks from file\n",
        "    with open(FAIL_TASKS_FILE, 'r') as f:\n",
        "        failed_ids = [line.strip() for line in f if line.strip()]\n",
        "    \n",
        "    # Build paths (add .json if needed)\n",
        "    all_failed = []\n",
        "    for task_id in failed_ids:\n",
        "        if not task_id.endswith('.json'):\n",
        "            task_id = task_id + '.json'\n",
        "        path = os.path.join(EVAL_DIR, task_id)\n",
        "        if os.path.exists(path):\n",
        "            all_failed.append(path)\n",
        "    \n",
        "    print(f\"âœ“ Found {len(all_failed)}/{len(failed_ids)} valid tasks in fail_tasks.txt\")\n",
        "    \n",
        "    if not all_failed:\n",
        "        print(\"âŒ No valid failed tasks found!\")\n",
        "        return []\n",
        "    \n",
        "    # Random sample\n",
        "    random.seed()  # Use current time for randomness\n",
        "    selected = random.sample(all_failed, min(NUM_TASKS, len(all_failed)))\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ðŸ”„ RETRYING FAILED TASKS ({len(selected)} tasks)\")\n",
        "    print(f\"   Total failed: {len(all_failed)} tasks\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nðŸ“‹ Selected: {[os.path.basename(p) for p in selected[:5]]}...\")\n",
        "    \n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    # Run tasks\n",
        "    results = await run_tasks(selected, verbose=True)\n",
        "    \n",
        "    duration = (datetime.now() - start_time).total_seconds()\n",
        "    \n",
        "    # Summary using official ARC scoring\n",
        "    total_tests = sum(r.n_test_cases for r in results)\n",
        "    total_correct = sum(r.n_correct for r in results)\n",
        "    tasks_fully_solved = sum(1 for r in results if r.score == 1.0)\n",
        "    errors = sum(1 for r in results if r.error)\n",
        "    avg_score = sum(r.score for r in results) / len(results) if results else 0\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ðŸ† RETRY RESULTS (Official ARC Scoring)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ðŸ“‹ Tasks evaluated: {len(results)}\")\n",
        "    print(f\"ðŸ“‹ Total test cases: {total_tests}\")\n",
        "    print(f\"âœ… Test cases correct: {total_correct}/{total_tests} ({100*total_correct/total_tests:.1f}%)\")\n",
        "    print(f\"ðŸŽ¯ Tasks fully solved: {tasks_fully_solved}/{len(results)}\")\n",
        "    print(f\"ðŸ“Š Average task score: {avg_score*100:.1f}%\")\n",
        "    print(f\"âŒ Errors: {errors}\")\n",
        "    print(f\"â±ï¸  Total time: {duration:.0f}s ({duration/len(results):.1f}s per task avg)\")\n",
        "    \n",
        "    print(\"\\nðŸ“‹ Individual results:\")\n",
        "    for r in results:\n",
        "        status = \"âœ“\" if r.score == 1.0 else \"â—\" if r.score > 0 else \"âœ—\"\n",
        "        tests_info = f\" ({r.n_correct}/{r.n_test_cases} tests)\" if r.n_test_cases > 1 else \"\"\n",
        "        print(f\"   {status} {r.task_id}: {r.score*100:.0f}%{tests_info}\")\n",
        "    \n",
        "    # Show which previously failed tasks now pass\n",
        "    newly_passing = [r.task_id for r in results if r.score == 1.0]\n",
        "    if newly_passing:\n",
        "        print(f\"\\nðŸŽ‰ Fully solved: {newly_passing}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run it!\n",
        "results = asyncio.get_event_loop().run_until_complete(run_failed_tasks())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
